{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chao2/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import urllib, json\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "from mxnet import nd, init, autograd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import gluon\n",
    "from time import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data\n",
    "train_input_temp = pd.read_csv('EI_training_input_with_header.csv',header=None)\n",
    "train_input_temp2 = pd.read_csv('EI_training_input.csv',header=None)\n",
    "\n",
    "#train_output_temp = pd.read_csv('EI_training_output_data.csv')\n",
    "train_output_temp2 = pd.read_csv('EI_training_output.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_input_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input=nd.array(train_input_temp2,mx.gpu())\n",
    "#train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 1.]\n",
       " [0. 0. 1.]]\n",
       "<NDArray 2x3 @gpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output=nd.array(train_output_temp2,mx.gpu())#\n",
    "train_output[[1,5],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.00341021  0.00367314 -0.00375737]\n",
       " [ 0.00521525  0.00640165 -0.00486244]]\n",
       "<NDArray 2x3 @gpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = mx.gpu()\n",
    "layer = nn.Sequential()\n",
    "\n",
    "with layer.name_scope():\n",
    "    layer.add(        \n",
    "        nn.Dense(6, activation=\"relu\"),\n",
    "        nn.Dropout(0.5),          # dropout to ease the overfitting problem\n",
    "        nn.Dense(3)\n",
    "    )\n",
    "layer.initialize(ctx=ctx)\n",
    "x = nd.random.uniform(0,1,(2,train_input.shape[1]),ctx=ctx)\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "l2loss = gluon.loss.L2Loss()\n",
    "kldivloss = gluon.loss.KLDivLoss(from_logits=False)\n",
    "trainer = gluon.Trainer(layer.collect_params(),\n",
    "                        'adam', {'learning_rate': 0.1,'wd': 0.0001}) # 'weight decay to ease the overfitting problem\n",
    "#trainer = gluon.Trainer(layer.collect_params(), 'sgd',\n",
    "                            #{'learning_rate': 0.1, 'momentum': 0.95})\n",
    "train_loss, train_acc, valid_acc = 0., 0., 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed2=[[0,0,1],[0,1,0],[1,0,0]]\n",
    "def acc(output, label):\n",
    "    # output: (batch, num_output) float32 ndarray\n",
    "    # label: (batch, ) int32 ndarray\n",
    "    return (output.argmax(axis=1) ==\n",
    "            label.argmax(axis=1)).mean().asscalar()\n",
    "def acc2(output, label):\n",
    "    # output: (batch, num_output) float32 ndarray\n",
    "    # label: (batch, ) int32 ndarray\n",
    "    return (output.argmax(axis=1) -\n",
    "            label.argmax(axis=1)).abs().mean().asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tain number 36 over 44\n",
      "[[23, 26, 22, 9, 6, 8, 15, 34, 29, 3, 33, 25, 2, 20, 19, 16, 24, 1, 18, 7, 0, 12, 27, 10, 5, 30, 32, 35, 21, 31, 4, 14, 28, 17, 11, 13]]\n"
     ]
    }
   ],
   "source": [
    "train_number = 36\n",
    "print('tain number', train_number,'over',train_input.shape[0])\n",
    "batchsize=36\n",
    "sampler = gluon.data.RandomSampler(train_number)\n",
    "batch_sampler = gluon.data.BatchSampler(sampler, batchsize, 'keep')\n",
    "print(list(batch_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0: Loss: 0.36577, Acc. Train 0.472, Acc. Test 0.625, Time 0.0 sec\n",
      "# 1: Loss: 0.34623, Acc. Train 0.472, Acc. Test 0.625, Time 0.0 sec\n",
      "# 2: Loss: 0.36417, Acc. Train 0.472, Acc. Test 0.625, Time 0.0 sec\n",
      "# 3: Loss: 0.33220, Acc. Train 0.472, Acc. Test 0.625, Time 0.0 sec\n",
      "# 4: Loss: 0.31502, Acc. Train 0.667, Acc. Test 0.500, Time 0.0 sec\n",
      "# 5: Loss: 0.31584, Acc. Train 0.694, Acc. Test 0.500, Time 0.0 sec\n",
      "# 6: Loss: 0.32984, Acc. Train 0.694, Acc. Test 0.500, Time 0.0 sec\n",
      "# 7: Loss: 0.28488, Acc. Train 0.667, Acc. Test 0.500, Time 0.0 sec\n",
      "# 8: Loss: 0.27904, Acc. Train 0.667, Acc. Test 0.625, Time 0.0 sec\n",
      "# 9: Loss: 0.29561, Acc. Train 0.694, Acc. Test 0.500, Time 0.0 sec\n",
      "# 10: Loss: 0.28004, Acc. Train 0.806, Acc. Test 0.625, Time 0.0 sec\n",
      "# 11: Loss: 0.27220, Acc. Train 0.833, Acc. Test 0.750, Time 0.0 sec\n",
      "# 12: Loss: 0.28692, Acc. Train 0.806, Acc. Test 0.750, Time 0.0 sec\n",
      "# 13: Loss: 0.28091, Acc. Train 0.694, Acc. Test 0.375, Time 0.0 sec\n",
      "# 14: Loss: 0.25644, Acc. Train 0.722, Acc. Test 0.375, Time 0.0 sec\n",
      "# 15: Loss: 0.32204, Acc. Train 0.778, Acc. Test 0.750, Time 0.0 sec\n",
      "# 16: Loss: 0.30827, Acc. Train 0.722, Acc. Test 0.625, Time 0.0 sec\n",
      "# 17: Loss: 0.28455, Acc. Train 0.639, Acc. Test 0.625, Time 0.0 sec\n",
      "# 18: Loss: 0.30567, Acc. Train 0.639, Acc. Test 0.625, Time 0.0 sec\n",
      "# 19: Loss: 0.25021, Acc. Train 0.639, Acc. Test 0.625, Time 0.0 sec\n",
      "# 20: Loss: 0.30594, Acc. Train 0.639, Acc. Test 0.625, Time 0.0 sec\n",
      "# 21: Loss: 0.26349, Acc. Train 0.528, Acc. Test 0.625, Time 0.0 sec\n",
      "# 22: Loss: 0.27962, Acc. Train 0.528, Acc. Test 0.625, Time 0.0 sec\n",
      "# 23: Loss: 0.26196, Acc. Train 0.722, Acc. Test 0.750, Time 0.0 sec\n",
      "# 24: Loss: 0.26755, Acc. Train 0.694, Acc. Test 0.750, Time 0.0 sec\n",
      "# 25: Loss: 0.25800, Acc. Train 0.750, Acc. Test 0.750, Time 0.0 sec\n",
      "# 26: Loss: 0.23484, Acc. Train 0.778, Acc. Test 0.750, Time 0.0 sec\n",
      "# 27: Loss: 0.27034, Acc. Train 0.778, Acc. Test 0.750, Time 0.0 sec\n",
      "# 28: Loss: 0.28112, Acc. Train 0.806, Acc. Test 0.750, Time 0.0 sec\n",
      "# 29: Loss: 0.22719, Acc. Train 0.833, Acc. Test 0.750, Time 0.0 sec\n",
      "# 30: Loss: 0.26443, Acc. Train 0.750, Acc. Test 0.375, Time 0.0 sec\n",
      "# 31: Loss: 0.25963, Acc. Train 0.750, Acc. Test 0.375, Time 0.0 sec\n",
      "# 32: Loss: 0.25151, Acc. Train 0.806, Acc. Test 0.750, Time 0.0 sec\n",
      "# 33: Loss: 0.27009, Acc. Train 0.806, Acc. Test 0.750, Time 0.0 sec\n",
      "# 34: Loss: 0.22495, Acc. Train 0.806, Acc. Test 0.750, Time 0.0 sec\n",
      "# 35: Loss: 0.21620, Acc. Train 0.806, Acc. Test 0.750, Time 0.0 sec\n",
      "# 36: Loss: 0.30890, Acc. Train 0.806, Acc. Test 0.750, Time 0.0 sec\n",
      "# 37: Loss: 0.23930, Acc. Train 0.806, Acc. Test 0.750, Time 0.0 sec\n",
      "# 38: Loss: 0.23966, Acc. Train 0.833, Acc. Test 0.750, Time 0.0 sec\n",
      "# 39: Loss: 0.23215, Acc. Train 0.833, Acc. Test 0.750, Time 0.0 sec\n",
      "# 40: Loss: 0.26663, Acc. Train 0.778, Acc. Test 0.375, Time 0.0 sec\n",
      "# 41: Loss: 0.29941, Acc. Train 0.861, Acc. Test 0.750, Time 0.0 sec\n",
      "# 42: Loss: 0.27901, Acc. Train 0.861, Acc. Test 0.750, Time 0.0 sec\n",
      "# 43: Loss: 0.19700, Acc. Train 0.889, Acc. Test 0.750, Time 0.0 sec\n",
      "# 44: Loss: 0.28326, Acc. Train 0.861, Acc. Test 0.750, Time 0.0 sec\n",
      "# 45: Loss: 0.23092, Acc. Train 0.833, Acc. Test 0.750, Time 0.0 sec\n",
      "# 46: Loss: 0.23132, Acc. Train 0.833, Acc. Test 0.750, Time 0.0 sec\n",
      "# 47: Loss: 0.25442, Acc. Train 0.833, Acc. Test 0.625, Time 0.0 sec\n",
      "# 48: Loss: 0.28324, Acc. Train 0.833, Acc. Test 0.750, Time 0.0 sec\n",
      "# 49: Loss: 0.25968, Acc. Train 0.806, Acc. Test 0.625, Time 0.0 sec\n",
      "# 50: Loss: 0.24424, Acc. Train 0.806, Acc. Test 0.625, Time 0.0 sec\n",
      "# 51: Loss: 0.27126, Acc. Train 0.806, Acc. Test 0.625, Time 0.0 sec\n",
      "# 52: Loss: 0.23906, Acc. Train 0.861, Acc. Test 0.750, Time 0.0 sec\n",
      "# 53: Loss: 0.21453, Acc. Train 0.861, Acc. Test 0.750, Time 0.0 sec\n",
      "# 54: Loss: 0.22953, Acc. Train 0.861, Acc. Test 0.750, Time 0.0 sec\n",
      "# 55: Loss: 0.21415, Acc. Train 0.833, Acc. Test 0.750, Time 0.0 sec\n",
      "# 56: Loss: 0.25110, Acc. Train 0.833, Acc. Test 0.750, Time 0.0 sec\n",
      "# 57: Loss: 0.27363, Acc. Train 0.833, Acc. Test 0.750, Time 0.0 sec\n",
      "# 58: Loss: 0.26050, Acc. Train 0.833, Acc. Test 0.750, Time 0.0 sec\n",
      "# 59: Loss: 0.22698, Acc. Train 0.889, Acc. Test 0.750, Time 0.0 sec\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(60):\n",
    "    train_loss, train_acc, valid_acc = 0., 0., 0.\n",
    "    tic = time()\n",
    "    cumulative_loss=0\n",
    "    for b in list(batch_sampler):\n",
    "        #print(b)\n",
    "        with autograd.record():\n",
    "            output = layer(train_input[b,:])\n",
    "            loss = kldivloss(output, train_output[b,:])\n",
    "            #loss = softmax_cross_entropy(output,train_output[b,:])\n",
    "        loss.backward()\n",
    "        train_loss+=loss.mean().asscalar()\n",
    "        trainer.step(batchsize)\n",
    "    # update parameters\n",
    "    # calculate training metrics\n",
    "    train_acc = acc(layer(train_input[0:train_number,:]), train_output[0:train_number,:])\n",
    "\n",
    "    # calculate validation accuracy\n",
    "    valid_acc = acc(layer(train_input[train_number:,:]), train_output[train_number:,:])\n",
    "\n",
    "    print(\"# %d: Loss: %.5f, Acc. Train %.3f, Acc. Test %.3f, Time %.1f sec\" % (\n",
    "         epoch, train_loss,\n",
    "         train_acc,\n",
    "         valid_acc,\n",
    "         time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: Loss: 0.36589, Acc. Train 0.444, Acc. Test 0.714, Time 0.0 sec\n",
    "# 1: Loss: 0.35063, Acc. Train 0.444, Acc. Test 0.714, Time 0.0 sec\n",
    "# 2: Loss: 0.36259, Acc. Train 0.500, Acc. Test 0.714, Time 0.0 sec\n",
    "# 3: Loss: 0.33976, Acc. Train 0.556, Acc. Test 0.857, Time 0.0 sec\n",
    "# 4: Loss: 0.33622, Acc. Train 0.556, Acc. Test 0.429, Time 0.0 sec\n",
    "# 5: Loss: 0.33215, Acc. Train 0.694, Acc. Test 0.429, Time 0.0 sec\n",
    "# 6: Loss: 0.31903, Acc. Train 0.806, Acc. Test 0.714, Time 0.0 sec\n",
    "# 7: Loss: 0.30061, Acc. Train 0.694, Acc. Test 1.000, Time 0.0 sec\n",
    "# 8: Loss: 0.28325, Acc. Train 0.694, Acc. Test 0.714, Time 0.0 sec\n",
    "# 9: Loss: 0.26440, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 10: Loss: 0.24668, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 11: Loss: 0.23730, Acc. Train 0.778, Acc. Test 0.714, Time 0.0 sec\n",
    "# 12: Loss: 0.22289, Acc. Train 0.722, Acc. Test 0.857, Time 0.0 sec\n",
    "# 13: Loss: 0.21040, Acc. Train 0.750, Acc. Test 0.857, Time 0.0 sec\n",
    "# 14: Loss: 0.20170, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 15: Loss: 0.19095, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 16: Loss: 0.18502, Acc. Train 0.694, Acc. Test 0.714, Time 0.0 sec\n",
    "# 17: Loss: 0.17856, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 18: Loss: 0.17327, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 19: Loss: 0.17310, Acc. Train 0.722, Acc. Test 0.714, Time 0.0 sec\n",
    "# 20: Loss: 0.17011, Acc. Train 0.806, Acc. Test 0.571, Time 0.0 sec\n",
    "# 21: Loss: 0.16822, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 22: Loss: 0.16849, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 23: Loss: 0.16630, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 24: Loss: 0.16510, Acc. Train 0.722, Acc. Test 0.714, Time 0.0 sec\n",
    "# 25: Loss: 0.16239, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 26: Loss: 0.16102, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 27: Loss: 0.15868, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 28: Loss: 0.15671, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 29: Loss: 0.15614, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 30: Loss: 0.15362, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 31: Loss: 0.15171, Acc. Train 0.778, Acc. Test 0.714, Time 0.0 sec\n",
    "# 32: Loss: 0.14932, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 33: Loss: 0.14809, Acc. Train 0.778, Acc. Test 0.714, Time 0.0 sec\n",
    "# 34: Loss: 0.14746, Acc. Train 0.778, Acc. Test 0.714, Time 0.0 sec\n",
    "# 35: Loss: 0.14706, Acc. Train 0.806, Acc. Test 0.571, Time 0.0 sec\n",
    "# 36: Loss: 0.14664, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 37: Loss: 0.14592, Acc. Train 0.750, Acc. Test 0.714, Time 0.0 sec\n",
    "# 38: Loss: 0.14584, Acc. Train 0.778, Acc. Test 0.714, Time 0.0 sec\n",
    "# 39: Loss: 0.14441, Acc. Train 0.806, Acc. Test 0.714, Time 0.0 sec\n",
    "# 40: Loss: 0.14368, Acc. Train 0.806, Acc. Test 0.714, Time 0.0 sec\n",
    "# 41: Loss: 0.14253, Acc. Train 0.778, Acc. Test 0.714, Time 0.0 sec\n",
    "# 42: Loss: 0.14097, Acc. Train 0.806, Acc. Test 0.714, Time 0.0 sec\n",
    "# 43: Loss: 0.13925, Acc. Train 0.861, Acc. Test 0.571, Time 0.0 sec\n",
    "# 44: Loss: 0.13772, Acc. Train 0.861, Acc. Test 0.571, Time 0.0 sec\n",
    "# 45: Loss: 0.13668, Acc. Train 0.889, Acc. Test 0.571, Time 0.0 sec\n",
    "# 46: Loss: 0.13575, Acc. Train 0.889, Acc. Test 0.571, Time 0.0 sec\n",
    "# 47: Loss: 0.13470, Acc. Train 0.861, Acc. Test 0.571, Time 0.0 sec\n",
    "# 48: Loss: 0.13335, Acc. Train 0.806, Acc. Test 0.714, Time 0.0 sec\n",
    "# 49: Loss: 0.13194, Acc. Train 0.833, Acc. Test 0.714, Time 0.0 sec\n",
    "# 50: Loss: 0.13045, Acc. Train 0.833, Acc. Test 0.714, Time 0.0 sec\n",
    "# 51: Loss: 0.12892, Acc. Train 0.833, Acc. Test 0.714, Time 0.0 sec\n",
    "# 52: Loss: 0.12766, Acc. Train 0.833, Acc. Test 0.714, Time 0.0 sec\n",
    "# 53: Loss: 0.12623, Acc. Train 0.833, Acc. Test 0.714, Time 0.0 sec\n",
    "# 54: Loss: 0.12491, Acc. Train 0.889, Acc. Test 0.571, Time 0.0 sec\n",
    "# 55: Loss: 0.12360, Acc. Train 0.889, Acc. Test 0.571, Time 0.0 sec\n",
    "# 56: Loss: 0.12216, Acc. Train 0.861, Acc. Test 0.571, Time 0.0 sec\n",
    "# 57: Loss: 0.12042, Acc. Train 0.861, Acc. Test 0.571, Time 0.0 sec\n",
    "# 58: Loss: 0.11782, Acc. Train 0.889, Acc. Test 0.571, Time 0.0 sec\n",
    "# 59: Loss: 0.11521, Acc. Train 0.889, Acc. Test 0.571, Time 0.0 sec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.save_parameters('audit_deltaEI.params')\n",
    "train_input_temp.to_pickle(\"audit_deltaEI.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['UZB', 'ZAF', 'ISR', 'MAR', 'KHM', 'SEN']: 'audit_deltaEI_095_1.params', \"audit_deltaEI_095_1.data\";# 49: Loss: 0.21741, Acc. Train 0.950, Acc. Test 1.000, Time 0.0 sec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
